# 2022 Business Analytics Topic 1 Tutorial
__2022010558 ê¹€ì§€í˜„__  
1. __Supervised Method__ ì¤‘ ë³€ìˆ˜ ì„ íƒ ê¸°ë²• ì¤‘ í•˜ë‚˜ì¸ [Genetic Algorithm(ìœ ì „ ì•Œê³ ë¦¬ì¦˜)ì— ëŒ€í•œ íŠœí† ë¦¬ì–¼](https://github.com/Im-JihyunKim/BusinessAnalytics_Topic1/blob/main/Supervised%20Dimensionality%20Reduction/GA_Feature_Selection_Tutorial.ipynb)ì„ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.   
2. __Unsupervised Method__ ì¤‘ Linear Embedding ê¸°ë²• ì¤‘ [MDS(ë‹¤ì°¨ì› ì²™ë„ë²•)ì— ëŒ€í•œ íŠœí† ë¦¬ì–¼](https://github.com/Im-JihyunKim/BusinessAnalytics_Topic1/blob/main/Unsupervised%20Dimensionality%20Reduction/MDS_Tutorial.ipynb)ì„ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.  
ë§í¬ë¥¼ í´ë¦­í•˜ë©´ ë³´ë‹¤ ìƒì„¸í•œ íŠœí† ë¦¬ì–¼ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Table of Contents:
- [Dimensionality Reduction](#dimensionality-reduction)
  * [Supervised Dimensionality Redcution: Genetic Algorithm](#supervised-dimensionality-redcution--genetic-algorithm)
    + [How Genetic Algorithm Works](#how-genetic-algorithm-works)
    + [Genetic Algorithm Ending Criteria](#genetic-algorithm-ending-criteria)
    + [Fitness Evaluation](#fitness-evaluation)
    + [Selection](#selection)
    + [Crossover and Mutation](#crossover-and-mutation)
    + [Requirements](#requirements)
    + [Parameters](#parameters)
    + [Argparse](#argparse)
    + [Example of Use](#example-of-use)
  * [Multidimensional Reduction (MDS)](#multidimensional-reduction--mds-)
    + [Purpose](#purpose)
    + [How to Use](#how-to-use)
    + [Parameters](#parameters-1)
    + [Simple Illustration](#simple-illustration)
- [References](#references)
    + [Genetic Algorithm](#genetic-algorithm)
    + [Multidimensional Scaling](#multidimensional-scaling)

# Dimensionality Reduction
ì´ë¯¸ì§€, í…ìŠ¤íŠ¸, ì„¼ì„œ ë“± ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ ë°ì´í„°ë“¤ì€ ë³€ìˆ˜ì˜ ìˆ˜ê°€ ë§¤ìš° ë§ì€ ê³ ì°¨ì› ë°ì´í„°(High Dimensional Data)ì˜ íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë§ì€ ê¸°ê³„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì€ ì‹¤ì œ ë°ì´í„° ì°¨ì›ì„ ëª¨ë‘ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì •ë³´ë¥¼ ì¶•ì•½í•˜ì—¬ ë‚´ì¬ëœ ì°¨ì›(Intrinsic/Embedded Dimension)ì„ í™œìš©í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ëŠ” __ì°¨ì›ì˜ ì €ì£¼(curse of Dimensionality)__ ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•¨ì¸ë°, ì‚¬ìš©í•˜ëŠ” ë³€ìˆ˜ ìˆ˜ë¥¼ ì¤„ì´ë©´ ì¡ìŒ(noise)ì´ í¬í•¨ë  í™•ë¥ ë„ ê°ì†Œì‹œí‚´ê³¼ ë™ì‹œì— ì˜ˆì¸¡ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì´ê³ , ì˜ˆì¸¡ ëª¨ë¸ì˜ í•™ìŠµê³¼ ì¸ì‹ ì†ë„ë¥¼ ë¹ ë¥´ê²Œ í•  ìˆ˜ ìˆìœ¼ë©° ì˜ˆì¸¡ ëª¨ë¸ì— í•„ìš”í•œ í•™ìŠµ ì§‘í•©ì˜ í¬ê¸°ë¥¼ í¬ê²Œ í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.   
ë”°ë¼ì„œ ë¶„ì„ ê³¼ì •ì—ì„œ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¤ì§€ ì•ŠëŠ” ìµœì†Œí•œì˜ ë³€ìˆ˜ ì§‘í•©ì„ íŒë³„í•˜ì—¬ ì£¼ìš” ì •ë³´ë§Œì„ ë³´ì¡´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë©°, ì°¨ì› ì¶•ì†Œ ë°©ì‹ì€ __(1) Supervised Dimensionality Reduction (êµì‚¬ì  ì°¨ì› ì¶•ì†Œ)__ , __(2) Unupservised Deimensionality Reduction (ë¹„êµì‚¬ì  ì°¨ì› ì¶•ì†Œ)__ ë‘ ê°€ì§€ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Supervised Dimensionality Reductionì€ ì¶•ì†Œëœ ì°¨ì›ì˜ ì í•©ì„±ì„ ê²€ì¦í•˜ëŠ” ë° ìˆì–´ ì˜ˆì¸¡ ëª¨ë¸ì„ ì ìš©í•˜ë©°, ë™ì¼í•œ ë°ì´í„°ë¼ë„ ì ìš©ë˜ëŠ” ëª¨ë¸ì— ë”°ë¼ ì„ë² ë”© ê²°ê³¼ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤. ë°˜ë©´ Unupservised Deimensionality Reductionì€ ì¶•ì†Œëœ ì°¨ì›ì˜ ì í•©ì„±ì„ ê²€ì¦í•˜ëŠ” ë° ìˆì–´ ì˜ˆì¸¡ ëª¨ë¸ì„ ì ìš©í•˜ì§€ ì•Šê³ , íŠ¹ì • ê¸°ë²•ì— ë”°ë¼ì„œ ì°¨ì› ì¶•ì†Œ ê²°ê³¼ëŠ” ì–¸ì œë‚˜ ë™ì¼í•˜ë‹¤ëŠ” íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤.  
ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” __Supervised Dimensionality Reduction__ ë°©ë²•ë¡  ì¤‘ í™œìš©í•˜ëŠ” ë³€ìˆ˜ì˜ ìˆ˜ë¥¼ ì¤„ì´ëŠ” __Feature Selection (ë³€ìˆ˜ ì„ íƒ)__ ë°©ë²•ë¡  ì¤‘ __Genetic Algorithm (ìœ ì „ ì•Œê³ ë¦¬ì¦˜)ì— ì´ˆì ì„ ë§ì¶”ì–´ ì°¨ì› ì¶•ì†Œë¥¼ ìˆ˜í–‰__ í•´ë³´ê³ ì í•©ë‹ˆë‹¤. ë˜í•œ __Unupservised Deimensionality Reduction__ ë°©ë²•ë¡  ì¤‘ì—ì„œëŠ” Linear Embedding ë°©ë²•ë¡  ì¤‘ __Multidimensional Scaling (ë‹¤ì°¨ì› ì²™ë„ë²•)__ ì— ì´ˆì ì„ ë§ì¶”ì–´ íŠœí† ë¦¬ì–¼ì„ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.

## Supervised Dimensionality Redcution: Genetic Algorithm
ìœ ì „ ì•Œê³ ë¦¬ì¦˜ì€ ë³€ìˆ˜ ì„ íƒ ê¸°ë²• ì¤‘ ê°€ì¥ ìš°ìˆ˜í•œ ë°©ë²•ì…ë‹ˆë‹¤. ì´ì „ê¹Œì§€ì˜ ë³€ìˆ˜ ì„ íƒ ê¸°ë²•ë“¤ì€ íƒìƒ‰ ì†Œìš” ì‹œê°„ì„ ì¤„ì—¬ íš¨ìœ¨ì ì¸ ë°©ë²•ë¡ ì„ ì œì•ˆí•˜ì˜€ìœ¼ë‚˜, íƒìƒ‰ ë²”ìœ„ê°€ ì ì–´ Global Optimumì„ ì°¾ì„ í™•ë¥ ì´ ì ì€ í•œê³„ë¥¼ ê°€ì§€ê³  ìˆì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ __ìì—°ê³„ì˜ ì§„í™” ì²´ê³„ë¥¼ ëª¨ë°©í•œ ë©”íƒ€ íœ´ë¦¬ìŠ¤í‹± ì•Œê³ ë¦¬ì¦˜__ ì¸ GAëŠ” ì‹œí–‰ì°©ì˜¤ë¥¼ í†µí•´ ìµœì ì˜ í•´ë¥¼ ì°¾ì•„ë‚˜ê°€ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œ, ë‹¤ìœˆì˜ ìì—° ì„ íƒì„¤ì— ê¸°ë°˜í•˜ì—¬ ì´ˆê¸°ì— ë‹¤ì–‘í•œ ìœ ì „ìë¥¼ ê°€ì§€ê³  ìˆë˜ ì¢…ì´ ìƒì¡´ì— ìœ ë¦¬í•œ ìœ ì „ìë¥¼ íƒí•˜ë©´ì„œ í˜„ì¬ ìƒíƒœê°€ ë˜ì—ˆë‹¤ëŠ” ì´ë¡ ì„ ë”°ë¼ í•´ë¥¼ ìµœì í™” í•´ë‚˜ê°‘ë‹ˆë‹¤.
> **Heuristic íœ´ë¦¬ìŠ¤í‹±**   
> ì°¸ê³ ë¡œ íœ´ë¦¬ìŠ¤í‹±ì´ë€ ë¶ˆì¶©ë¶„í•œ ì‹œê°„ì´ë‚˜ ì •ë³´ë¡œ ì¸í•˜ì—¬ í•©ë¦¬ì ì¸ íŒë‹¨ì„ í•  ìˆ˜ ì—†ê±°ë‚˜, ì²´ê³„ì ì´ë©´ì„œ í•©ë¦¬ì ì¸ íŒë‹¨ì´ êµ³ì´ í•„ìš”í•˜ì§€ ì•Šì€ ìƒí™©ì—ì„œ ì‚¬ëŒë“¤ì´ ë¹ ë¥´ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë³´ë‹¤ ìš©ì´í•˜ê²Œ êµ¬ì„±ëœ ê°„í¸ì¶”ë¡  ë°©ë²•ë¡ ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. **ë©”íƒ€ íœ´ë¦¬ìŠ¤í‹±(Meta Heuristic)** ì€ íœ´ë¦¬ìŠ¤í‹± ë°©ë²•ë¡  ì¤‘ í’€ì´ ê³¼ì • ë“±ì´ êµ¬ì¡°ì ìœ¼ë¡œ ì˜ ì •ì˜ë˜ì–´ ìˆì–´ ëŒ€ë¶€ë¶„ì˜ ë¬¸ì œì— ì–´ë ¤ì›€ ì—†ì´ ì ìš©í•  ìˆ˜ ìˆëŠ” íœ´ë¦¬ìŠ¤í‹±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

### How Genetic Algorithm Works
ìœ ì „ ì•Œê³ ë¦¬ì¦˜ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ í•´ë¡œ êµ¬ì„±ëœ ì ì¬ í•´ ì§‘ë‹¨ì„ ë§Œë“¤ê³  ì í•©ë„(fitness)ë¥¼ í‰ê°€í•œ ë’¤, ì¢‹ì€ í•´ë¥¼ ì„ ë³„í•´ì„œ ìƒˆë¡œìš´ í•´ ì§‘ë‹¨(í›„ê¸° ì„¸ëŒ€)ì„ ë§Œë“œëŠ” ë©”íƒ€ íœ´ë¦¬ìŠ¤í‹± ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ì§„í™” ì´ë¡  ì¤‘ ìì—°ì„ íƒì„¤ì— ê¸°ë°˜í•˜ì—¬ ì„¸ëŒ€ë¥¼ ìƒì„±í•´ë‚´ë©°, ì£¼ì–´ì§„ ë¬¸ì œë¥¼ ì˜ í’€ê¸° ìœ„í•œ ìµœì í•´ë¥¼ ì°¾ê±°ë‚˜ ì¢…ë£Œ ì¡°ê±´ì„ ë§Œì¡± ì‹œ ì•Œê³ ë¦¬ì¦˜ì´ ì¢…ë£Œë©ë‹ˆë‹¤. í›„ê¸° ì„¸ëŒ€ë¥¼ ë§Œë“œëŠ” ê³¼ì •ì€ (ë¶€ëª¨ ì„¸ëŒ€) __ì„ íƒ(Selection)__ , __êµë°°(Corssover)__ , __ëŒì—°ë³€ì´ ë°œìƒ(Mutation)__ 3ê°€ì§€ì— ê¸°ë°˜í•˜ë©°, í•œ ì„¸ëŒ€(ì ì¬í•´ ì§‘ë‹¨)ëŠ” __ì í•©ë„ í•¨ìˆ˜(Fitness function)__ ì— ì˜í•´ ë¬¸ì œ í•´ê²°ì— ì í•©í•œì§€ í‰ê°€ë©ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ Genetic Algorithmì€ ìµœì í™” ë¬¸ì œì—ì„œ ì‚¬ìš©ë˜ì§€ë§Œ, ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ì°¨ì› ì¶•ì†Œ ì‹œ ëª©í‘œë³€ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì„¤ëª… ë³€ìˆ˜ ì¡°í•©ì„ ì„ íƒí•˜ëŠ” ë° Genetic Algorithmì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ë³¸ ì•Œê³ ë¦¬ì¦˜ì„ ë„ì‹í™”í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
![image](https://user-images.githubusercontent.com/115214552/195269445-768a0a06-c8ad-43a4-9a1a-a4d0c5d331fb.png)

__Genetic Algorithm Process:__
1. ì´ˆê¸° ì„¸ëŒ€ ìƒì„±
2. ì„¸ëŒ€ ì í•©ë„ í‰ê°€([Fitness Evaluation](#fitness-evaluation))
3. ë¶€ëª¨ ì„¸ëŒ€ ì„ íƒ([Selection](#selection))
4. êµì°¨ ë° ëŒì—°ë³€ì´ ìƒì„±ì„ í†µí•œ ìì‹ ì„¸ëŒ€ ìƒì„±([Crossover & Mutation](#crossover-and-mutation))
5. ìì‹ ì„¸ëŒ€ ì í•©ë„ í‰ê°€

### Genetic Algorithm Ending Criteria
ìœ ì „ ì•Œê³ ë¦¬ì¦˜ì€ ìœ„ 5ë‹¨ê³„ Processë¥¼ ê±°ì¹˜ë©° ì•Œê³ ë¦¬ì¦˜ ì¢…ë£Œ ì¡°ê±´ì„ ë§Œì¡±í•œ ê²½ìš° í•™ìŠµì´ ì™„ë£Œë©ë‹ˆë‹¤.
1. ì‚¬ìš©ìê°€ ì§€ì •í•œ ì„¸ëŒ€ ìˆ˜(`n_generation`)ë¥¼ ëª¨ë‘ ìƒì„±í•œ ê²½ìš°
2. í•™ìŠµ ì‹œ ëª¨ë¸ì´ ìˆ˜ë ´í•œ ê²½ìš°
 - ì´ëŠ” `threshold_times_convergence` íšŸìˆ˜ë¥¼ ë„˜ì–´ê°€ëŠ” ë™ì•ˆ ìµœê³  ì„±ëŠ¥ì„ ê°±ì‹ í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ì— í•´ë‹¹í•©ë‹ˆë‹¤. ì¦‰, ì•Œê³ ë¦¬ì¦˜ì´ local optimalì„ ì°¾ì•„ ì¢…ë£Œëœ ê²½ìš°ì…ë‹ˆë‹¤. `threshold_times_convergence`ëŠ” ì´ˆê¸°ì— 5ë²ˆìœ¼ë¡œ ìƒì •í•˜ì˜€ìŠµë‹ˆë‹¤.
 -  ë§Œì¼ `n_genetation`ì˜ ì ˆë°˜ ì´ìƒ í•™ìŠµì´ ì§„í–‰ë˜ì—ˆë‹¤ë©´ ì¡°ê¸ˆ ë” ì¦ê°€í•˜ì—¬ global optimalì„ ì°¾ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤. `threshold_times_convergence`ë¥¼ ìƒì„±ëœ ì„¸ëŒ€ ìˆ˜ì˜ 30% ë§Œí¼ìœ¼ë¡œ ì§€ì •í•˜ì—¬, í•´ë‹¹ ìˆ˜ ì´ìƒìœ¼ë¡œ Score ê°’ì´ ì¼ì •í•˜ë‹¤ë©´ í•™ìŠµì„ ì¢…ë£Œí•©ë‹ˆë‹¤.
 -  ë”ë¶ˆì–´ ìƒˆë¡œìš´ ìì‹ ì„¸ëŒ€ì˜ ìµœê³  ì„±ëŠ¥ê³¼ ì „ ì„¸ëŒ€ì˜ ìµœê³  ì„±ëŠ¥ ê°„ ì°¨ì´ê°€ ì§€ì •í•œ `threshold` ë³´ë‹¤ ë‚®ë‹¤ë©´, `threshold_times_convergence` íšŸìˆ˜ë§Œí¼ ë°˜ë³µë  ê²½ìš° í•™ìŠµì„ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤.

### Fitness Evaluation
- ì í•©ë„ í‰ê°€ëŠ” ê° ì—¼ìƒ‰ì²´(Chromosome)ì˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµëœ ëª¨í˜•ì˜ ì í•©ë„ë¥¼ í‰ê°€í•˜ëŠ”ë°, ì—¼ìƒ‰ì²´ì˜ ìš°ì—´ì„ ê°€ë¦´ ìˆ˜ ìˆëŠ” ì •ëµì  ì§€í‘œë¥¼ í†µí•´ì„œ ë†’ì€ ê°’ì„ ê°€ì§ˆ ìˆ˜ë¡ ìš°ìˆ˜í•œ ì—¼ìƒ‰ì²´(ë³€ìˆ˜ ì¡°í•©)ìœ¼ë¡œì„œ ì±„íƒí•©ë‹ˆë‹¤.
- ì í•©ë„ í•¨ìˆ˜(Fitness Function)ê°€ ê°€ì ¸ì•¼ í•˜ëŠ” ë‘ ê°€ì§€ ì¡°ê±´ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
    1. ë‘ ì—¼ìƒ‰ì²´ê°€ __ë™ì¼í•œ ì˜ˆì¸¡ ì„±ëŠ¥__ ì„ ë‚˜íƒ€ë‚¼ ê²½ìš°, __ì ì€ ìˆ˜ì˜ ë³€ìˆ˜__ ë¥¼ ì‚¬ìš©í•œ ì—¼ìƒ‰ì²´ ì„ í˜¸
    2. ë‘ ì—¼ìƒ‰ì²´ê°€ __ë™ì¼í•œ ë³€ìˆ˜__ ë¥¼ ì‚¬ìš©í–ˆì„ ê²½ìš°, __ìš°ìˆ˜í•œ ì˜ˆì¸¡ ì„±ëŠ¥__ ì„ ë‚˜íƒ€ë‚´ëŠ” ì—¼ìƒ‰ì²´ ì„ í˜¸
- ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œ Classification Taskë¥¼ ìœ„í•´ ì‚¬ìš©í•œ ëª¨ë¸ì€ Logistic Regressionì´ë©°, ì í•©ë„ í‰ê°€ë¥¼ ìœ„í•´ ì‚¬ìš©í•œ ì²™ë„ëŠ” __(1) Accuracy__ , __(2) F1-Score__ , __(3) AUROC Score__ 3ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤. Regression Taskë¥¼ ìœ„í•´ ì‚¬ìš©í•œ ëª¨ë¸ì€ Linear Regression í˜¹ì€ ë‹¤ë¥¸ ì–´ë–¤ ëª¨ë¸ì´ë“  ìƒê´€ ì—†ìœ¼ë©°, ì í•©ë„ í‰ê°€ë¥¼ ìœ„í•´ ì‚¬ìš©í•œ ì²™ë„ëŠ”  __(1) ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜, (2) MAPE, (2) RMSE, (4) MAEë¥¼ 1ì—ì„œ ë¹¼ì¤€ ê°’__ ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

### Selection
- ì í•©ë„ í•¨ìˆ˜ë¥¼ í†µí•´ì„œ ë¶€ëª¨ ì—¼ìƒ‰ì²´ì˜ ìš°ìˆ˜ì„±ì„ í‰ê°€í•˜ì˜€ë‹¤ë©´, Step 3ì—ì„œëŠ” ìš°ìˆ˜í•œ ë¶€ëª¨ ì—¼ìƒ‰ì²´ë¥¼ ì„ íƒí•˜ì—¬ ìì†ì—ê²Œ ë¬¼ë ¤ì¤ë‹ˆë‹¤. ì´ëŠ” ë¶€ëª¨ ì—¼ìƒ‰ì²´ê°€ ìš°ì›”í•˜ë‹¤ë©´, ìì†ë“¤ë„ ìš°ì›”í•  ê²ƒì´ë¼ëŠ” ê°€ì •ì— ê¸°ë°˜í•©ë‹ˆë‹¤. ì´ë•Œ ë¶€ëª¨ ì—¼ìƒ‰ì²´ë¥¼ ì„ íƒí•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ ê°€ì§€ì´ê³ , ëŒ€í‘œì ì¸ ë°©ë²•ë¡ ë“¤ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
    1. __Deterministic Selection__  
       - ì í•©ë„ í‰ê°€ ê²°ê³¼ë¡œ ì‚°ì¶œëœ rank ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ N%ì˜ ì—¼ìƒ‰ì²´ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ìš°ìˆ˜í•œ ìœ ì „ìë¥¼ ë¬¼ë ¤ì£¼ì–´ ì¢‹ì€ í•´ë¥¼ ë§Œë“¤ì–´ë‚´ê¸° ìœ„í•œ ë°©ë²•ë¡ ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìƒìœ„ N%ë³´ë‹¤ ì•„ë˜ì˜ ì—¼ìƒ‰ì²´ ì¤‘ ì í•©ë„ì— ì°¨ì´ê°€ ì–¼ë§ˆ ë‚˜ì§€ ì•ŠëŠ” ê²½ìš°ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•œë‹¤ëŠ” í•œê³„ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ë¥¼ ë³´ì™„í•œ ë°©ë²•ì´ Probabilistic Selectionì…ë‹ˆë‹¤.
    2. __Probabilistic Selection__
       - ê° ì—¼ìƒ‰ì²´ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬, ëª¨ë“  ì—¼ìƒ‰ì²´ì—ê²Œ ìì†ì—ê²Œ ì „ë‹¬í•´ ì¤„ ìˆ˜ ìˆëŠ” ê¸°íšŒë¥¼ ë¶€ì—¬í•˜ëŠ” ë°©ë²•ë¡ ì…ë‹ˆë‹¤. ë£°ë › íœ  ë°©ì‹(Roulette Wheel Selection)ì´ë¼ê³ ë„ í•˜ë©°, Classification Taskì—ì„œëŠ” Softmax í™•ë¥  ê°’ì— ê¸°ë°˜í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    3. __Tournament Selection__
       - ë¬´ì‘ìœ„ë¡œ Kê°œì˜ ì—¼ìƒ‰ì²´ë¥¼ ì„ íƒí•˜ê³ , ì´ë“¤ ì¤‘ ê°€ì¥ ìš°ìˆ˜í•œ ì—¼ìƒ‰ì²´ë¥¼ íƒí•˜ì—¬ ë‹¤ìŒ ì„¸ëŒ€ë¡œ ì „ë‹¬í•˜ëŠ” ë°©ë²•ë¡ ì…ë‹ˆë‹¤. ë™ì¼í•œ í”„ë¡œì„¸ìŠ¤ê°€ ë‹¤ìŒ ìƒìœ„ ì—¼ìƒ‰ì²´ë¥¼ ì„ íƒí•˜ê¸° ìœ„í•´ ë°˜ë³µë˜ë©°, Deterministic Selectionì˜ ë‹¨ì ì„ ì–´ëŠì •ë„ ë³´ì™„í•œ ë™ì‹œì— ì—°ì‚° ì‹œê°„ì´ ë¹„êµì  ì§§ë‹¤ëŠ” ì¥ì ì„ ê°€ì§‘ë‹ˆë‹¤.
- ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ì—¼ìƒ‰ì²´ ì„¸ëŒ€ê°€ ì–¸ì œë‚˜ ë™ì¼í•´ì•¼ í•œë‹¤ëŠ” ì ì— ê¸°ë°˜í•˜ì—¬, __Tournament Selectionì„ ì´ìš©í•˜ì—¬ ì„ íƒì„ ì§„í–‰__ í•˜ì˜€ìŠµë‹ˆë‹¤.  ê°€ì¥ ì í•©ë„ê°€ ë†’ì€ ì—¼ìƒ‰ì²´ë¥¼ ì„ ì •í•œ ì´í›„ì—, ë¬´ì‘ìœ„ë¡œ Kê°œì˜ ì—¼ìƒ‰ì²´ë¥¼ ê³¨ë¼ ì í•©ë„ Scoreë¥¼ ë¹„êµí•˜ê³ , ë†’ì€ ì—¼ìƒ‰ì²´ë¥¼ ê³ ë¥´ëŠ” ê³¼ì •ì„ ì„¸ëŒ€ ìˆ˜ë§Œí¼ ë°˜ë³µí•˜ì—¬ ë‹¤ìŒ ì„¸ëŒ€ë¥¼ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤. í•´ë‹¹ ë°©ë²•ë¡ ì˜ ê°œìš”ë¥¼ ë„ì‹í™” í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

<p align="center">
     <img src="https://user-images.githubusercontent.com/115214552/195280272-066c04b5-fe53-4fdb-9ead-6e81edfd5f9b.png" alt="tournament selection"/>
</p>

### Crossover and Mutation
![image](https://user-images.githubusercontent.com/115214552/195280032-02f005bd-48ae-4221-a4cc-98056240fc71.png)

__Crossover êµë°°__
- ì„ íƒëœ ë¶€ëª¨ ì—¼ìƒ‰ì²´ë¡œë¶€í„° ìì‹ì„¸ëŒ€ë¥¼ ì¬ìƒì‚°í•´ë‚´ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. 
- ì• ë‹¨ê³„ì—ì„œ ì„ íƒëœ ë¶€ëª¨ ì—¼ìƒ‰ì²´ë“¤ì˜ ìœ ì „ì ì •ë³´ë¥¼ ì„œë¡œ êµí™˜í•˜ì—¬ ìƒˆë¡œìš´ ìì‹ ì—¼ìƒ‰ì²´ë“¤ì„ ìµœì¢…ì ìœ¼ë¡œ ìƒì„±í•´ëƒ…ë‹ˆë‹¤.
- ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” êµë°°ìœ¨ì„ Hyperparameterë¡œ ì§€ì •í•˜ì—¬, ì–¼ë§ˆë‚˜ ë§ì€ ë³€ìˆ˜ë“¤ì„ êµí™˜í•˜ì—¬ ìì‹ ì—¼ìƒ‰ì²´ë¥¼ ìƒì„±í•´ë‚¼ ì§€ë¥¼ ììœ ë¡­ê²Œ ì§€ì •í•  ìˆ˜ ìˆê²Œ í•˜ì˜€ìŠµë‹ˆë‹¤.
- ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œ ì‚¬ìš©ëœ êµë°°ìœ¨(crossover_rate)ì€ 0.7ì…ë‹ˆë‹¤.

__Mutation ëŒì—°ë³€ì´__
- ëŒì—°ë³€ì´ëŠ” ì„¸ëŒ€ê°€ ì§„í™”í•´ ê°€ëŠ” ê³¼ì •ì—ì„œ ë‹¤ì–‘ì„±ì„ í™•ë³´í•˜ê¸° ìœ„í•œ ì¥ì¹˜ì…ë‹ˆë‹¤.
- íŠ¹ì • ìœ ì „ìì˜ ì •ë³´ë¥¼ ë‚®ì€ í™•ë¥ ë¡œ ë°˜ëŒ€ ê°’ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ í†µí•´ ëŒì—°ë³€ì´ë¥¼ ìœ ë„í•©ë‹ˆë‹¤.
- ëŒì—°ë³€ì´ë¥¼ í†µí•´ í˜„ì¬ í•´ê°€ Local Optimumì—ì„œ íƒˆì¶œí•  ìˆ˜ ìˆëŠ” ê¸°íšŒë¥¼ ì œê³µí•˜ì§€ë§Œ, ë„ˆë¬´ ë†’ì€ ëŒì—°ë³€ì´ìœ¨ì€ ìœ ì „ ì•Œê³ ë¦¬ì¦˜ì˜ convergence ì†ë„ë¥¼ ëŠ¦ì¶”ê¸°ì— ì£¼ë¡œ 0.01 ì´í•˜ì˜ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.


### Requirements
- Python >= 3.6
- numpy >= 1.18
- pandas >= 1.0.1
- rich >= 12.6.0
- scikit-learn >= 1.1.2

### Parameters
Genetic Algorithm classë¥¼ í˜¸ì¶œí•˜ëŠ” ë° í•„ìš”í•œ íŒŒë¼ë¯¸í„° ëª©ë¡ì…ë‹ˆë‹¤.
|__Parameter__|__Type__|__Default__|__Definition__|
|------|---|---|---|
|`model`|object||Scikit-learnì—ì„œ ì œê³µí•˜ëŠ” ê¸°ë³¸ ì§€ë„í•™ìŠµ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì´ì–´ì•¼ í•©ë‹ˆë‹¤. fit, predict ë“±ì˜ methodë¥¼ ì§€ì›í•´ì•¼ í•©ë‹ˆë‹¤.|
|`args`|argparse||ìœ ì „ ì•Œê³ ë¦¬ì¦˜ì— í•„ìš”í•œ ì—¬ëŸ¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.|
|`seed`|int|2022|ê° ì„¸ëŒ€ë¥¼ ë§Œë“¤ì–´ëƒ„ì— ìˆì–´ Randomnessë¥¼ ì œì–´í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. ì •ìˆ˜ê°’ì„ ì…ë ¥í•©ë‹ˆë‹¤.|

### Argparse
ìœ ì „ ì•Œê³ ë¦¬ì¦˜ì—ì„œ í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ëª©ë¡ì…ë‹ˆë‹¤. í„°ë¯¸ë„ì—ì„œ `main.py`ë¥¼ ì‹¤í–‰ ì‹œ ì¸ì ê°’ì„ ììœ ë¡­ê²Œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
|__Argument__|__Type__|__Default__|__Help__|
|------|---|---|---|
|`seed`|int|2022|ê° ì„¸ëŒ€ë¥¼ ë§Œë“¤ì–´ëƒ„ì— ìˆì–´ Randomnessë¥¼ ì œì–´í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. ì •ìˆ˜ê°’ì„ ì…ë ¥í•©ë‹ˆë‹¤.|
|`normalization`|bool|False|ì…ë ¥ ë°ì´í„° ê°’ Scaling ì—¬ë¶€ì…ë‹ˆë‹¤.|
|`n_generation`|int|50|ì–¼ë§ˆë‚˜ ë§ì€ ì„¸ëŒ€ë¥¼ ë§Œë“¤ì–´ë‚¼ ì§€ë¥¼ ê²°ì •í•˜ëŠ” ë¶€ë¶„ìœ¼ë¡œ, ì•Œê³ ë¦¬ì¦˜ ì¢…ë£Œì¡°ê±´ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.|
|`n_population`|int|100|í•œ ì„¸ëŒ€ì— ì–¼ë§ˆë‚˜ ë§ì€ ì—¼ìƒ‰ì²´ ìˆ˜(ë³€ìˆ˜ ì¡°í•©)ë¥¼ ê³ ë ¤í•  ê²ƒì¸ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. ê°’ì´ í´ ìˆ˜ë¡ ì—°ì‚°ëŸ‰ì´ ë§ì•„ì§€ì§€ë§Œ ë” ë§ì€ ë²”ìœ„ë¥¼ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.|
|`crossover_rate`|float|0.7|ìœ ì „ì ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ êµí™˜í•˜ì—¬ ìì‹ ì„¸ëŒ€ë¥¼ ìƒì„±í•  ì§€ ë¹„ìœ¨ì„ ì§€ì •í•©ë‹ˆë‹¤. 0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ê°’ì„ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.|
|`mutation_rate`|float|0.1|ìì‹ ì„¸ëŒ€ì—ì„œ ëŒì—°ë³€ì´ë¥¼ ì–¼ë§ˆë‚˜ ë§Œë“¤ì–´ë‚¼ ì§€ë¥¼ ë¹„ìœ¨ì„ ì§€ì •í•©ë‹ˆë‹¤. 0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ê°’ì„ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.|
|`tournament_k`|int|2|ë³¸ íŠœí† ë¦¬ì–¼ì€ Selection ì‹œ Tournament Selection ë°©ì‹ì„ íƒí–ˆìŠµë‹ˆë‹¤. ë¶€ëª¨ ì„¸ëŒ€ë¡œ ì„ íƒí•˜ê¸° ìœ„í•œ ê³¼ì • ì¤‘ Kê°œì˜ ì—¼ìƒ‰ì²´ë¥¼ ë¬´ì‘ìœ„ë¡œ ê³¨ë¼ í† ë„ˆë¨¼íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.|
|`c_metric`|str|'accuracy'|Classification Taskì—ì„œì˜ ì í•©ë„ í‰ê°€ë¥¼ ìœ„í•œ ì§€í‘œì…ë‹ˆë‹¤. accuracy, f1-score, roc_auc_score 3ê°€ì§€ë¥¼ ì„ íƒí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.|
|`r_metric`|str|'rmse'|Regression Taskì—ì„œì˜ ì í•©ë„ í‰ê°€ë¥¼ ìœ„í•œ ì§€í‘œì…ë‹ˆë‹¤. corr, rmse, mape, mae 4ê°€ì§€ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.|
|`n_jobs`|int|1|CPU ì½”ì–´ë¥¼ ì–¼ë§ˆë‚˜ ì‚¬ìš©í•  ì§€ë¥¼ ì •í•˜ëŠ” ì¸ìì…ë‹ˆë‹¤. -1ë¡œ ì§€ì • ì‹œ ì»´í“¨í„°ì˜ ëª¨ë“  ì½”ì–´ë¥¼ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤.|
|`initial_best_chromosome`|np.ndarray|None|1ì°¨ì›ì˜ ì´ì§„í™”ëœ ë§¤íŠ¸ë¦­ìŠ¤ë¡œ, ë°ì´í„°ì˜ ë³€ìˆ˜ ê°œìˆ˜ ë§Œí¼ì˜ í¬ê¸°ë¥¼ ê°–ìŠµë‹ˆë‹¤. ì´ˆê¸° ì„¸ëŒ€ì—ì„œì˜ ìµœê³  ì—¼ìƒ‰ì²´ê°€ ë¬´ì—‡ì¸ì§€ë¥¼ ê²°ì •í•˜ëŠ” ì¸ìì…ë‹ˆë‹¤.|
|`verbose`|int|0|í•¨ìˆ˜ ìˆ˜í–‰ ì‹œ ì¶œë ¥ë˜ëŠ” ì •ë³´ë“¤ì„ ì–¼ë§ˆë‚˜ ìƒì„¸íˆ í•  ì§€ë¥¼ ê²°ì •í•˜ëŠ” ì¸ìì…ë‹ˆë‹¤. 0ì€ ì¶œë ¥í•˜ì§€ ì•Šê³ , 1ì€ ìì„¸íˆ, 2ëŠ” í•¨ì¶•ì  ì •ë³´ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.|

### Example of Use
```python
import argparse
import numpy as np
import rich
import argparse
from ga_feature_selection.genetic_algorithm import GA_FeatureSelector
from sklearn import datasets
from sklearn.datasets import make_classification, make_regression
from sklearn import linear_model


def main(args):
    """Loading X(features), y(targets) from datasets"""
    data = datasets.load_wine()
    X, y = data['data'], data['targets']
    LogisticRegression = linear_model.LogisticRegression(solver='lbfgs', multi_class='auto')
    Genetic_Algorithm = GA_FeatureSelector(model=LogisticRegression, args=args, seed=args.seed)
    
    """Making train and test set"""
    X_train, X_test, y_train, y_test = Genetic_Algorithm.data_prepare(X, y)
    Genetic_Algorithm.run(X_train, X_test, y_train, y_test)

    """Show the result"""
    table, summary_table = Genetic_Algorithm.summary_table()
    rich.print(table)
    rich.print(summary_table)




if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--seed", default=2022, type=int)
    parser.add_argument("--normalization", default=False, type=bool)
    parser.add_argument("--n-generation", default=10, type=int, help="Determines the maximum number of generations to be carry out.")
    parser.add_argument("--n-population", default=100, type=int, help="Determines the size of the population (number of chromosomes).")
    parser.add_argument("--crossover-rate", default=0.7, type=float, help="Defines the crossing probability. It must be a value between 0.0 and 1.0.")
    parser.add_argument("--mutation-rate", default=0.1, type=float, help="Defines the mutation probability. It must be a value between 0.0 and 1.0.")
    parser.add_argument("--tournament-k", default=2, type=int, help="Defines the size of the tournament carried out in the selection process. \n 
                         Number of chromosomes facing each other in each tournament.")
    parser.add_argument("--n-jobs", default=1, choices=[1, -1], type=int, help="Number of cores to run in parallel. By default a single-core is used.")
    parser.add_argument("--initial-best-chromosome", default=None, type=np.ndarray, 
                        help="A one-dimensional binary matrix of size equal to the number of features (M). \n
                        Defines the best chromosome (subset of features) in the initial population.")
    parser.add_argument("--verbose", default=0, type=int, help="Control the output verbosity level. It must be an integer value between 0 and 2.")
    parser.add_argument("--c-metric", default='accuracy', choices=['accuracy', 'f1_score', 'roc_auc_socre'], type=str)
    parser.add_argument("--r-metric", default='rmse', choices=['rmse', 'corr', 'mape', 'mae'], type=str)
    
    args = parser.parse_args()
    
    main(args)
```
```
Creating initial population with 100 chromosomes ğŸ§¬
 âœ” Evaluating initial population...
 âœ” Current best chromosome: [1 0 0 0 0 1 1 0 0 1 0 1 1], Score: 0.971830985915493
Creating generation 1...
 âœ” Evaluating population of new generation 1...
 âœ” (Better) A better chromosome than the current one has been found 0.9859154929577465
 âœ” Current best chromosome: [1 1 1 1 0 1 1 1 1 1 0 1 0], Score: 0.9859154929577465
    Elapsed generation time:  2.73 seconds
Creating generation 2...
 âœ” Evaluating population of new generation 2...
 âœ” Same scoring value found 1 / 5 times.
 âœ” Current best chromosome: [1 1 1 1 0 1 1 1 1 1 0 1 0], Score: 0.9859154929577465
    Elapsed generation time:  2.71 seconds
Creating generation 3...
 âœ” Evaluating population of new generation 3...
 âœ” Same scoring value found 2 / 5 times.
 âœ” Current best chromosome: [1 1 1 1 0 1 1 1 1 1 0 1 0], Score: 0.9859154929577465
    Elapsed generation time:  2.69 seconds
(...)
Creating generation 49...
 âœ” Evaluating population of new generation 49...
 âœ” (WORSE) No better chromosome than the current one has been found 0.971830985915493
 âœ” Current best chromosome: [1 0 1 1 0 0 1 0 0 0 1 0 0], Score: 0.9929577464788732
    Elapsed generation time:  2.76 seconds
Creating generation 50...
 âœ” Evaluating population of new generation 50...
 âœ” (WORSE) No better chromosome than the current one has been found 0.9788732394366197
 âœ” Current best chromosome: [1 0 1 1 0 0 1 0 0 0 1 0 0], Score: 0.9929577464788732
    Elapsed generation time:  2.71 seconds
Training time:  138.77 seconds
```

ê²°ê³¼(table, summary table)ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                  â”ƒ     Selected     â”ƒ                  â”ƒ                 â”ƒ                  â”ƒ   Training Time   â”ƒ
â”ƒ Best Chromosome  â”ƒ   Features ID    â”ƒ Best Test Score  â”ƒ Best Generation â”ƒ Best Train Score â”ƒ       (sec)       â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ [1 0 1 1 0 0 1 0 â”‚ [ 0  2  3  6 10] â”‚ 0.9929577464788â€¦ â”‚        4        â”‚       1.0        â”‚      138.77       â”‚
â”‚    0 0 1 0 0]    â”‚                  â”‚                  â”‚                 â”‚                  â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Number of Generation â”ƒ Number of Population â”ƒ Crossover Rate â”ƒ Mutation Rate â”ƒ  Metric  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”©
â”‚          50          â”‚         100          â”‚      0.7       â”‚      0.1      â”‚ accuracy â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Multidimensional Reduction (MDS)
MDSëŠ” ë°ì´í„°ë¥¼ ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ mapping í•¨ì— ìˆì–´ non-linear ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. ê³ ì°¨ì› ê³µê°„ì— ìˆëŠ” ì ì„ ì €ì°¨ì› ê³µê°„ì— mappingí•˜ë©´ì„œ, í•´ë‹¹ ì  ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ìµœëŒ€í•œ ìœ ì§€í•˜ëŠ” ê²ƒì´ MDSì˜ ëª©ì ì…ë‹ˆë‹¤. ì¦‰, ì €ì°¨ì› ê³µê°„ ìƒì—ì„œ ë°ì´í„°ì˜ Pairwise DistanceëŠ” ê³ ì°¨ì› ê³µê°„ì˜ ì‹¤ì œ ê±°ë¦¬ì™€ ê±°ì˜ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤. MDSëŠ” Classification ë° Regression Taskì—ì„œë„ ì „ì²˜ë¦¬ ë‹¨ê³„ ì°¨ì›ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ë°, ë³€ìˆ˜ë¥¼ ì¶•ì†Œí• ë¿ ì•„ë‹ˆë¼ ë°ì´í„°ë¥¼ ì‹œê°í™”í•¨ì— ìˆì–´ì„œë„ íš¨ê³¼ì ì¸ ê¸°ìˆ ì…ë‹ˆë‹¤. ì €ì°¨ì› ê³µê°„ì—ì„œë„ ê³ ì°¨ì›ì˜ ì›ë³¸ ë°ì´í„°ì™€ ë™ì¼í•œ Clusterì™€ Patternì„ ìœ ì§€í•˜ê¸° ë•Œë¬¸ì—, ì¼ë¡€ë¡œ 5ì°¨ì›ì˜ ë°ì´í„°ê°€ ìˆë‹¤ê³  í•˜ë”ë¼ë„ 3ì°¨ì› ë°ì´í„°ë¡œ ë§Œë“¤ì–´ ì‹œê°í™”í•  ìˆ˜ ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.  
ì¼ë°˜ì ìœ¼ë¡œ MDSì—ì„œ ë°ì´í„° ê°„ Pairwise Distanceë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì€ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ë¥¼ ì´ìš©í•˜ì§€ë§Œ, ë‹¤ë¥¸ ì ì ˆí•œ metricì„ ì´ìš©í•˜ì—¬ ë¹„êµí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Scikit-learn ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ì„œ, pythonì„ ì´ìš©í•´ ë‹¤ì°¨ì› ì²™ë„ë²•ì„ êµ¬í˜„í•´ë³´ê³ ì í•©ë‹ˆë‹¤. ê°„ë‹¨í•œ ì˜ˆì œë¥¼ í†µí•´ì„œ MDS ì ìš© ë°©ë²•ë¡ ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

### Purpose
MDSëŠ” Dì°¨ì›ì˜ ê³µê°„ ìƒì— ê°ì²´ë“¤(ë°ì´í„°)ì´ ìˆë‹¤ê³  í–ˆì„ ë•Œ, í•´ë‹¹ ê°ì²´ë“¤ì˜ ê±°ë¦¬ê°€ ì €ì°¨ì› ê³µê°„ ìƒì—ì„œë„ ìµœëŒ€í•œ ë§ì´ ë³´ì¡´ë˜ë„ë¡ í•˜ëŠ” ì¶•, ì¢Œí‘œê³„ë¥¼ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤.  
ì•„ë˜ ì˜ˆì‹œë¥¼ ë³´ê² ìŠµë‹ˆë‹¤. ë§Œì¼ ë¯¸êµ­ì˜ ë‘ ë„ì‹œë“¤ ê°„ì˜ ë¹„í–‰ ê±°ë¦¬ë¥¼ í†µí•´ì„œ ê° ë„ì‹œë“¤ì´ ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ ìˆëŠ”ì§€ë¥¼ ê³„ì‚°í•œ Distance Matrixê°€ ì£¼ì–´ì ¸ ìˆë‹¤ ê°€ì •í•˜ê² ìŠµë‹ˆë‹¤. í•´ë‹¹ ì˜ˆì— ë”°ë¥´ë©´ ë³´ìŠ¤í„´ê³¼ ë‰´ìš•ì€ 206, ë³´ìŠ¤í„´ê³¼ DCëŠ” 409, ë³´ìŠ¤í„´ê³¼ ë§ˆì´ì• ë¯¸ëŠ” 1504 ë§Œí¼ ê±°ë¦¬ ì°¨ì´ê°€ ìˆìœ¼ë©°, ì´ì²˜ëŸ¼ ë‘ ê°ì²´ë“¤ ê°„ì— Pairwise Distanceë¥¼ 2ì°¨ì› ê³µê°„ ìƒì— ê° ë„ì‹œë“¤ì„ Mappingí•˜ë©´ 2ì°¨ì› ì¶•, ì¢Œí‘œë¡œ í‘œí˜„ë  ìˆ˜ ì‡ì„ ê²ƒì…ë‹ˆë‹¤. ê²°êµ­ MDSëŠ” ì´ëŸ¬í•œ Distance/Similarity Matrixë¥¼ í†µí•´ì„œ ì €ì°¨ì› ìƒì˜ ê° ê°ì²´ë“¤ì´ ê°–ëŠ” ì¢Œí‘œ(Coordinates) ì‹œìŠ¤í…œì„ ì°¾ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•©ë‹ˆë‹¤. ì´ëŠ” ì£¼ì„±ë¶„ ë¶„ì„(PCA)ì´ ë°ì´í„° íŠ¹ì§•ì„ ë°ì´í„°ê°€ ê°€ì§€ëŠ” ë¶„ì‚°ìœ¼ë¡œ ì •ì˜í•œ ê²ƒê³¼ëŠ” ë¶„ëª…íˆ ë‹¤ë¥¸ ì§€ì ì…ë‹ˆë‹¤.
![image](https://user-images.githubusercontent.com/115214552/195580072-a3a73167-9dd7-4f27-8cba-d2e3ed8e0132.png)

### How to Use
Scikit-Learn ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `sklearn.manifold` ëª¨ë“ˆì—ì„œëŠ” ë‹¤ì–‘í•œ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ embedding ê¸°ìˆ ì„ êµ¬í˜„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë•Œ í•´ë‹¹ ëª¨ë“ˆì—ì„œ ì œê³µí•˜ëŠ” 'MDS' í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì°¨ì› ì²™ë„ë²•ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
### Parameters
MDS Class ì‚¬ìš©ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ëª©ë¡ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
|__Parameter__|__Type__|__Default__|__Definition__|
|------|---|---|---|
|`n_components`|int|2|ë°ì´í„°ë¥¼ ëª‡ ì°¨ì›ìœ¼ë¡œ ì¤„ì¼ ì§€ë¥¼ ê²°ì •í•˜ëŠ” ì¸ìì…ë‹ˆë‹¤. ê¸°ë³¸ ê°’ì€ 2ì…ë‹ˆë‹¤.|
|`metric`|bool|True|Metric MDSì˜ ê²½ìš° True, Non-metric MDSì˜ ê²½ìš° Falseë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.|
|`dissimilarity`|str|'euclidean'|ê°ì²´ë“¤ ê°„ì˜ ê±°ë¦¬, ìœ ì‚¬ì„±, ë¹„ìœ ì‚¬ì„±ì„ êµ¬í•˜ëŠ” ì²™ë„ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤. í˜¹ì€ 'precomputed'ë¥¼ í†µí•´ ë¯¸ë¦¬ ê³„ì‚°ëœ Distance Matrixë¥¼ ì…ë ¥ ê°’ìœ¼ë¡œ í™œìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.|

### Simple Illustration
ì•„ë˜ì²˜ëŸ¼ ì„ì˜ì˜ 3ì°¨ì› ë°ì´í„°ì…‹ì´ ìˆë‹¤ê³  í•˜ì˜€ì„ ë•Œ, ì´ë¥¼ ì €ì°¨ì›(2ì°¨ì›)ìœ¼ë¡œ ì¶•ì†Œì‹œí‚¨ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.
- Scikit-learnì—ì„œ ì œê³µí•˜ëŠ” ë°©ì‹ì„ í†µí•´ ë°ì´í„°ë¥¼ ì¶•ì†Œí•˜ê³  ê·¸ ê²°ê³¼ë¥¼ ì‹œê°í™” í•œ ê²ƒì…ë‹ˆë‹¤.
- 2ì°¨ì› ê³µê°„ ìƒì—ì„œ mappingëœ ì¢Œí‘œë¥¼ ë³´ë©´, ì›ë³¸ ì°¨ì›ì—ì„œì˜ ë°ì´í„° í¬ì¸íŠ¸ ìƒì˜ ê±°ë¦¬ë¥¼ ê±°ì˜ ë™ì¼í•˜ê²Œ ìœ ì§€í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜¤ë Œì§€, ê°ˆìƒ‰, ë¶„í™ìƒ‰ì˜ í¬ì¸íŠ¸ë“¤ì´ ë§¤ìš° ê°€ê¹ê²Œ ëª°ë ¤ìˆëŠ” ê²ƒì´ ê·¸ ì˜ˆì…ë‹ˆë‹¤.
```Python
# Make dataset
X = np.random.uniform(1, 10, size=(10, 3))
rich.print('[black]Raw X data: ', '\n', f'[black]{X}')
```
```
Raw X data:  
 [[8.40240565 8.27954781 8.37334735]
 [7.23594263 1.27198244 3.75899031]
 [8.40197083 1.25961067 9.92239   ]
 [2.604193   7.08485018 9.5470354 ]
 [7.74940562 8.23301604 8.10162561]
 [3.58455736 9.95850387 8.86042996]
 [4.93951747 7.32528616 5.80387287]
 [6.41656317 3.58980413 8.47360454]
 [6.84648273 2.2235792  6.88919865]
 [7.4398821  8.90426094 7.02083169]]
```
```Python
# MDS Results using Euclidean Distance
mds = MDS(dissimilarity='precomputed', random_state=2022)
X_transform = mds.fit_transform(X)
rich.print('[black]MDS Coordinates: ', '\n', f'[black]{X_transform}')
```
```
MDS Coordinates:  
 [[-1.61079884 -2.95430259]
 [ 0.04169207  6.10767705]
 [-4.55096644  3.07620942]
 [ 4.40971489 -0.87276813]
 [-0.96938912 -2.6982923 ]
 [ 3.51228508 -3.87945677]
 [ 1.76924661 -0.74915278]
 [-1.16040954  1.78236135]
 [-1.19244475  3.49704443]
 [-0.24892997 -3.30931968]]
```

```Python
# Result Visualization
colors = ['darkorange', 'midnightblue', 'salmon', 'saddlebrown', 'peru',
          'darkcyan', 'indigo', 'darkseagreen', 'mediumseagreen', 'pink']
size = [64] * X.shape[0]
fig = plt.figure(2, (20, 10))
ax = fig.add_subplot(121, projection='3d')
plt.scatter(X[:,0], X[:,1], zs=X[:,2], s=size, c=colors)
plt.title('Original Points')

ax = fig.add_subplot(122)
plt.scatter(X_transform[:,0], X_transform[:,1], s=size, c=colors)
plt.title('Embedding in 2D')
fig.subplots_adjust(wspace=.4, hspace=0.5)
plt.show()
```
![output](https://user-images.githubusercontent.com/115214552/195580989-410e1870-0624-4cad-867c-fc697613536e.png)

# References
### Genetic Algorithm
- R. Tolosana, J.C. Ruiz-Garcia, R. Vera-Rodriguez, J. Herreros-Rodriguez, S. Romero-Tapiador, A. Morales and J. Fierrez, "Child-Computer Interaction: Recent Works, New Dataset, and Age Detection", IEEE Transactions on Emerging Topics in Computing, doi: 10.1109/TETC.2022.3150836, 2022.
- https://featureselectionga.readthedocs.io/en/latest/

### Multidimensional Scaling
- https://stackabuse.com/guide-to-multidimensional-scaling-in-python-with-scikit-learn/
- https://github.com/klyshko/ml_python/blob/master/Lecture9.ipynb
